# Logistic-Regression
Learning documentation for logistic regression

# Logistic Regression	ðŸ¤– ðŸ¤©
I. Classification	
- Email
- Online transaction ( fraudulent) yes or no?
- Tumor
- Credit risk

# Types
- Binary Classification
- Multiclass classification

## Binary Classification :
yâˆˆ {0,1}
## Multiclass Classification:
yâˆˆ{0,1,2,3}

# Ways of Applying  classification
- Linear Regression
- Logistic regression

## Linear Regression :
Assign threshold value using linear regression h(x) = Î¸tx , which will be used as a classifier.

## Logistic Regression
Want 0<=h(x)<=1 , so
Assign sigmoid function to classify the data point and optimising it .
h(x) = g(Î¸tx) = 1/1+e-(Î¸tx)
Here h(x) is estimated probability that y =1 on input x
 h(x) = p(y=1|x;Î¸) 
p(A) + P(B) = 1
Here, p(y=1|x;Î¸) + p(y=0|x;Î¸) = 1
So, p(y=0|x;Î¸) = 1 -  p(y=1|x;Î¸)

## Decision Boundary 
Letâ€™s predict y =1 if h(x) >= 0.5
And y = 0 if h(x) < 0.5
For y =1,
Since h(x) =g(z) , so â€”> g(z) >=0.5 when z>=0
hence, Î¸tx >=0
Letâ€™s consider an example with dataset has 2 training variable x1, x2

h(x) = g (Î¸0+Î¸1x1+Î¸2x2)
And letâ€™s consider we are finding 
â€”>Parameter as [-3 ;1;1]
Hence, to predict y = 1 , g(z) should be 
>= 0.5 , for that z should be >=0 
(note , here z  = Î¸0+Î¸1x1+Î¸2x2)
so, Î¸0+Î¸1x1+Î¸2x2 >=0
-3+x1+x2 >=0
 x1+x2 >= 3  ( which means y = 1 whenever the datapoint resides in the is area)
In another words , g(z)>= 0.5 when x1+x2 >= 3
Hence the boundary line which separates 2 different probability of event x1+x2 = 3

## How to choose Î¸ ?
Applying cost function method used in linear regression.
J(Î¸) = 1/m Î£ 1/2 (h(x) - y )2
But for logistic regression h(x) will be 1/1+e-z (which is non linear)
So that, J(Î¸) wonâ€™t converge to global minimum called â€˜non -convexâ€™
Hence â€”> this cost computing method canâ€™t be used

Logistic regression cost function is as follows
cost(h(x),y) = { -log(h(x) 	if y = 1
	 -log(1-h(x))	if y = 0
If h(x) = 1 and y =1 then J(Î¸) = 0
If h(x) = 1 and y =0 then J(Î¸) approaching âˆž

Intuition we are getting from the given cost is, if a hypothesis of rain predicted tomorrow h(x) = 1 but if it wonâ€™t rain y = 0 then we will penalise learning algorithm by very large cost. 

### â€”> Logistic Regression cost function
J(Î¸) = 1/m Î£ cost(h(x),y) 
here, cost(h(x),y) = { -log(h(x)) 	if y = 1
		                  -log(1-h(x))	if y = 0
                      
		OR
cost(h(x),y) = -y log(h(x)) - (1-y) log (1 - h(x))
â€”> want min J(Î¸) for function Î¸
To optimise theta , we gonna use gradient descent,
Repeat {
Î¸:=Î¸-Î± @/@Î¸ J(Î¸)
}

## Multiclass classification 
Email tagging
Climate forecast (Rain, Sunny, Windy)
Brain radio wave classification

Here we use one vs all methodology ,
Which means we will run binary classification for three times for training set which has three different classes.
Eg: Rain vs all , Sunny vs all , Windy vs all
And finally for a new input variable x to make prediction, we will pass the data into all 3 hypothesis and choose the one which give highest probability maxi h(x) 

## Fitting problem 
Some times based on the training set, they hypothesis we define may cause fitting problem for new example which is called â€˜biasâ€™ for under fitting and â€˜varianceâ€™ for overfitting. 
We will need to find the correct higher order polynomial for our training set so that it can give the most accurate prediction.


## Ways to reduce number of feature :
â€” Manually
â€” Using feature selection algorithm(like correlation)

## Regularisation :
â€” Keep all feature but reduce magnitude 
To avoid the overfitting problem, we may need to simplify the hypothesis value by reducing the parameter Î¸ for all the features(Except Î¸0 since it doesnâ€™t come up with feature) . To achieve that , we will do mathematical technique by adding â€˜Regularisation parameterâ€™ with existing cost function J(Î¸) as below.
J(Î¸) = 1/2m * [(sum (h(x) -y)^2) + Î»/m sumÎ¸j2]
Î»- Regularisation parameter
Here setting the value for Î» as minimum as possible may help to overcome the overfitting problem and will result the most accurate prediction.

**Note** : while applying gradient descent for regularised model, we will need to apply update rule for Î¸0 separately as it doesnâ€™t have regularised parameter added. And for rest of them with regularised parameter gradient descent has to applied .


                                            ***Thank You!***
